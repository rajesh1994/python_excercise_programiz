{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Engineering\n",
    "\n",
    "• The previous sections outline the fundamental ideas of machine learning, but all of the examples assume that you have numerical data in a tidy, [n_samples, n_features] format.\n",
    "\n",
    "• In the real world, data rarely comes in such a form. With this in mind, one of the more important steps in using machine learning in practice is feature engineering: that is, taking whatever information you have about your problem and turning it into numbers that you can use to build your feature matrix.\n",
    "\n",
    "• In this section, we will cover a few common examples of feature engineering tasks: features for representing categorical data, features for representing text, and features for representing images\n",
    "\n",
    "• Additionally, we will discuss derived features for increasing model complexity and imputation of missing data. \n",
    "\n",
    "• Often this process is known as vectorization, as it involves converting arbitrary data into well-behaved vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features\n",
    "\n",
    "• One common type of non-numerical data is categorical data.\n",
    "\n",
    "• For example, imagine you are exploring some data on housing prices, and along with numerical features like \"price\" and \"rooms\", you also have \"neighborhood\" information.\n",
    "\n",
    "• For example, your data might look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'price' : 850000, 'rooms' : 4, 'neighborhood' : 'Queen Anne'},\n",
    "    {'price' : 700000, 'rooms' : 3, 'neighborhood' : 'Fremont'},\n",
    "    {'price' : 650000, 'rooms' : 3, 'neighborhood' : 'Wallingford'},\n",
    "    {'price' : 600000, 'rooms' : 2, 'neighborhood' : 'Fremont'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• You might be tempted to encode this data with a straightforward numerical mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Queen Anne': 1, 'Fremont': 2, 'Wallingford': 3}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'Queen Anne' : 1, 'Fremont' : 2, 'Wallingford' : 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• It turns out that this is not generally a useful approach in Scikit-Learn: the package's models make the fundamental assumption that numerical features reflect algebraic quantities.\n",
    "\n",
    "• Thus such a mapping would imply, for example, that Queen Anne < Fremont < Wallingford, or even that Wallingford - Queen Anne = Fremont, which (niche demographic jokes aside) does not make much sense.\n",
    "\n",
    "• In this case, one proven technique is to use one-hot encoding, which effectively creates extra columns indicating the presence or absence of a category with a value of 1 or 0, respectively.\n",
    "\n",
    "• When your data comes as a list of dictionaries, Scikit-Learn's DictVectorizer will do this for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     0,      1,      0, 850000,      4],\n",
       "       [     1,      0,      0, 700000,      3],\n",
       "       [     0,      0,      1, 650000,      3],\n",
       "       [     1,      0,      0, 600000,      2]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vec = DictVectorizer(sparse=False, dtype=int)\n",
    "vec.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Notice that the 'neighborhood' column has been expanded into three separate columns, representing the three neighborhood labels, and that each row has a 1 in the column associated with its neighborhood.\n",
    "\n",
    "• With these categorical features thus encoded, you can proceed as normal with fitting a Scikit-Learn model.\n",
    "\n",
    "• To see the meaning of each column, you can inspect the feature names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neighborhood=Fremont',\n",
       " 'neighborhood=Queen Anne',\n",
       " 'neighborhood=Wallingford',\n",
       " 'price',\n",
       " 'rooms']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• There is one clear disadvantage of this approach: if your category has many possible values, this can greatly increase the size of your dataset.\n",
    "\n",
    "• However, because the encoded data contains mostly zeros, a sparse output can be a very efficient solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x5 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 12 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = DictVectorizer(sparse=True, dtype=int)\n",
    "vec.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Many (though not yet all) of the Scikit-Learn estimators accept such sparse inputs when fitting and evaluating models.\n",
    "\n",
    "• sklearn.preprocessing.OneHotEncoder and sklearn.feature_extraction.FeatureHasher are two additional tools that Scikit-Learn includes to support this type of encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Features\n",
    "\n",
    "• Another common need in feature engineering is to convert text to a set of representative numerical values.\n",
    "\n",
    "• For example, most automatic mining of social media data relies on some form of encoding the text as numbers.\n",
    "\n",
    "• One of the simplest methods of encoding data is by word counts: you take each snippet of text, count the occurrences of each word within it, and put the results in a table.\n",
    "\n",
    "• For example, consider the following set of three phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ['problem of evil',\n",
    "          'evil queen',\n",
    "          'horizon problem']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• For a vectorization of this data based on word count, we could construct a column representing the word \"problem,\" the word \"evil,\" the word \"horizon,\" and so on.\n",
    "\n",
    "• While doing this by hand would be possible, the tedium can be avoided by using Scikit-Learn's CountVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x5 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(sample)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• The result is a sparse matrix recording the number of times each word appears; it is easier to inspect if we convert this to a DataFrame with labeled columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evil</th>\n",
       "      <th>horizon</th>\n",
       "      <th>of</th>\n",
       "      <th>problem</th>\n",
       "      <th>queen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   evil  horizon  of  problem  queen\n",
       "0     1        0   1        1      0\n",
       "1     1        0   0        0      1\n",
       "2     0        1   0        1      0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(X.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• There are some issues with this approach, however: the raw word counts lead to features which put too much weight on words that appear very frequently, and this can be sub-optimal in some classification algorithms.\n",
    "\n",
    "• One approach to fix this is known as term frequency-inverse document frequency (TF–IDF) which weights the word counts by a measure of how often they appear in the documents.\n",
    "\n",
    "• The syntax for computing these features is similar to the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evil</th>\n",
       "      <th>horizon</th>\n",
       "      <th>of</th>\n",
       "      <th>problem</th>\n",
       "      <th>queen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.517856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.680919</td>\n",
       "      <td>0.517856</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.605349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.795961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.795961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.605349</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       evil   horizon        of   problem     queen\n",
       "0  0.517856  0.000000  0.680919  0.517856  0.000000\n",
       "1  0.605349  0.000000  0.000000  0.000000  0.795961\n",
       "2  0.000000  0.795961  0.000000  0.605349  0.000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer()\n",
    "X = vec.fit_transform(sample)\n",
    "pd.DataFrame(X.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Features\n",
    "\n",
    "• Another common need is to suitably encode images for machine learning analysis.\n",
    "\n",
    "• The simplest approach is what we used for the digits data in Introducing Scikit-Learn: simply using the pixel values themselves.\n",
    "\n",
    "• But depending on the application, such approaches may not be optimal.\n",
    "\n",
    "• A comprehensive summary of feature extraction techniques for images is well beyond the scope of this section, but you can find excellent implementations of many of the standard approaches in the Scikit-Image project.\n",
    "\n",
    "• For one example of using Scikit-Learn and Scikit-Image together, see Feature Engineering: Working with Images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derived Features\n",
    "\n",
    "• Another useful type of feature is one that is mathematically derived from some input features.\n",
    "\n",
    "• We saw an example of this in Hyperparameters and Model Validation when we constructed polynomial features from our input data.\n",
    "\n",
    "• We saw that we could convert a linear regression into a polynomial regression not by changing the model, but by transforming the input!\n",
    "\n",
    "• This is sometimes known as basis function regression, and is explored further in In Depth: Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• For example, this data clearly cannot be well described by a straight line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([4, 2, 1, 3, 7])\n",
    "plt.xlim(0, 6)\n",
    "plt.ylim(0, 8)\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Still, we can fit a line to the data using LinearRegression and get the optimal result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = x[:, np.newaxis]\n",
    "model = LinearRegression().fit(X, y)\n",
    "yfit = model.predict(X)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• It's clear that we need a more sophisticated model to describe the relationship between x and y.\n",
    "\n",
    "• One approach to this is to transform the data, adding extra columns of features to drive more flexibility in the model. For example, we can add polynomial features to the data this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
