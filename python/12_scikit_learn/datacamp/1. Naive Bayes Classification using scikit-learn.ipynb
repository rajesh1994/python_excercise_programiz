{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classification using Scikit-learn\n",
    "\n",
    "• Suppose you are a product manager, you want to classify customer reviews in positive and negative classes.\n",
    "\n",
    "• Or As a loan manager, you want to identify which loan applicants are safe or risky? As a healthcare analyst, you want to predict which patients can suffer from diabetes disease.\n",
    "\n",
    "• All the examples have the same kind of problem to classify reviews, loan applicants, and patients.\n",
    "\n",
    "• Naive Bayes is the most straightforward and fast classification algorithm, which is suitable for a large chunk of data.\n",
    "\n",
    "• Naive Bayes classifier is successfully used in various applications such as spam filtering, text classification, sentiment analysis, and recommender systems.\n",
    "\n",
    "• It uses Bayes theorem of probability for prediction of unknown class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Workflow\n",
    "\n",
    "• Whenever you perform classification, the first step is to understand the problem and identify potential features and label.\n",
    "\n",
    "• Features are those characteristics or attributes which affect the results of the label.\n",
    "\n",
    "• For example, in the case of a loan distribution, bank manager's identify customer’s occupation, income, age, location, previous loan history, transaction history, and credit score.\n",
    "\n",
    "• These characteristics are known as features which help the model classify customers.\n",
    "\n",
    "• The classification has two phases, a learning phase, and the evaluation phase.\n",
    "\n",
    "• In the learning phase, classifier trains its model on a given dataset and in the evaluation phase, it tests the classifier performance.\n",
    "\n",
    "• Performance is evaluated on the basis of various parameters such as accuracy, error, precision, and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Naive Bayes Classifier?\n",
    "\n",
    "• Naive Bayes is a statistical classification technique based on Bayes Theorem.\n",
    "\n",
    "• It is one of the simplest supervised learning algorithms.\n",
    "\n",
    "• Naive Bayes classifier is the fast, accurate and reliable algorithm.\n",
    "\n",
    "• Naive Bayes classifiers have high accuracy and speed on large datasets.\n",
    "\n",
    "• Naive Bayes classifier assumes that the effect of a particular feature in a class is independent of other features.\n",
    "\n",
    "• For example, a loan applicant is desirable or not depending on his/her income, previous loan and transaction history, age, and location.\n",
    "\n",
    "• Even if these features are interdependent, these features are still considered independently.\n",
    "\n",
    "• This assumption simplifies computation, and that's why it is considered as naive.\n",
    "\n",
    "• This assumption is called class conditional independence.\n",
    "\n",
    "                                    P(A/B) = P(B/A)P(A)/P(B)\n",
    "\n",
    "Where,\n",
    "\n",
    "    • P(A): the probability of hypothesis h being true (regardless of the data). This is known as the prior probability of h.\n",
    "\n",
    "    • P(B): the probability of the data (regardless of the hypothesis). This is known as the prior probability.\n",
    "\n",
    "    • P(A|B): the probability of hypothesis h given the data D. This is known as posterior probability.\n",
    "    \n",
    "    • P(B|A): the probability of data d given that the hypothesis h was true.  This is known as posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Naive Bayes classifier works?\n",
    "\n",
    "• Let’s understand the working of Naive Bayes through an example. Given an example of weather conditions and playing sports.\n",
    "\n",
    "• You need to calculate the probability of playing sports.\n",
    "\n",
    "• Now, you need to classify whether players will play or not, based on the weather condition.\n",
    "\n",
    "• Naive Bayes classifier calculates the probability of an event in the following steps:\n",
    "\n",
    "    Step 1: Calculate the prior probability for given class labels\n",
    "\n",
    "    Step 2: Find Likelihood probability with each attribute for each class\n",
    "\n",
    "    Step 3: Put these value in Bayes Formula and calculate posterior probability.\n",
    "\n",
    "    Step 4: See which class has a higher probability, given the input belongs to the higher probability class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Building in Scikit-learn\n",
    "\n",
    "<B><h3>Naive Bayes Classifier</B></h3>\n",
    "\n",
    "<h4><B>Defining Dataset</B></h4>\n",
    "\n",
    "• In this example, you can use the dummy dataset with three columns: weather, temperature, and play.\n",
    "\n",
    "• The first two are features(weather, temperature) and the other is the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning features and label variables\n",
    "weather=['Sunny','Sunny','Overcast','Rainy','Rainy','Rainy','Overcast','Sunny','Sunny',\n",
    "'Rainy','Sunny','Overcast','Overcast','Rainy']\n",
    "\n",
    "temp=['Hot','Hot','Hot','Mild','Cool','Cool','Cool','Mild','Cool','Mild','Mild','Mild','Hot','Mild']\n",
    "\n",
    "play=['No','No','Yes','Yes','Yes','No','Yes','No','Yes','Yes','Yes','Yes','Yes','No']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Features\n",
    "\n",
    "• First, you need to convert these string labels into numbers.\n",
    "\n",
    "• for example: 'Overcast', 'Rainy', 'Sunny' as 0, 1, 2.\n",
    "\n",
    "• This is known as label encoding.\n",
    "\n",
    "• Scikit-learn provides LabelEncoder library for encoding labels with a value between 0 and one less than the number of discrete classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 0 1 1 1 0 2 2 1 2 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "#creating labelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Converting string labels into numbers.\n",
    "weather_encoded=le.fit_transform(weather)\n",
    "print(weather_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Similarly, you can also encode temp and play columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp: [1 1 1 2 0 0 0 2 0 2 2 2 1 2]\n",
      "Play: [0 0 1 1 1 0 1 0 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Converting string labels into numbers\n",
    "temp_encoded=le.fit_transform(temp)\n",
    "label=le.fit_transform(play)\n",
    "print (\"Temp:\",temp_encoded)\n",
    "print (\"Play:\",label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Now combine both the features (weather and temp) in a single variable (list of tuples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1),\n",
       " (2, 1),\n",
       " (0, 1),\n",
       " (1, 2),\n",
       " (1, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (2, 2),\n",
       " (2, 0),\n",
       " (1, 2),\n",
       " (2, 2),\n",
       " (0, 2),\n",
       " (0, 1),\n",
       " (1, 2)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combinig weather and temp into single listof tuples\n",
    "features=list(zip(weather_encoded,temp_encoded))\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Model\n",
    "\n",
    "• Generate a model using naive bayes classifier in the following steps:\n",
    "\n",
    "    • Create naive bayes classifier\n",
    "\n",
    "    • Fit the dataset on classifier\n",
    "    \n",
    "    • Perform prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value: [1]\n"
     ]
    }
   ],
   "source": [
    "# Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create a Gaussian Classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model using the training sets\n",
    "model.fit(features,label)\n",
    "\n",
    "# Predict output\n",
    "predicted = model.predict([[1, 2]]) # 0 : Overcast, 1: Rainy, 2 : Mild\n",
    "print(\"Predicted Value:\", predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Here, 1 indicates that players can 'play'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
